# Request to gemini-3-flash-preview:

<message type=system>
  You are an expert software engineer. Your task is to extract a concise coding style guide from the provided code examples.
</message>

<message type=user>
  Based on these code examples, create a concise, clear coding style guide in Markdown format
  that captures the conventions used in this codebase, particularly the ones that leverage new or uncommon features.
  DO NOT repeat what are simply common best practices.
  
  <file path="frontend\index.html">
  <!doctype html>
  <html lang="en">
  
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" type="image/svg+xml" href="/logo-icon.svg" />
    <title>Nendo TikTok Bot | Intelligence Platform</title>
    <meta name="description" content="Nendo TikTok Social Listening & Intelligence Platform" />
    <meta name="author" content="Nendo" />
  
    <meta property="og:title" content="Nendo TikTok Bot" />
    <meta property="og:description" content="Nendo TikTok Social Listening & Intelligence Platform" />
    <meta property="og:type" content="website" />
  
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@Nendo" />
  </head>
  
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
  
  </html>
  </file>
  <file path="frontend\src\components\dashboard\Sidebar.tsx">
  import { useState } from 'react';
  import { cn } from '@/lib/utils';
  import {
    LayoutDashboard,
    Search,
    TrendingUp,
    Users,
    Hash,
    MessageSquare,
    Settings,
    ChevronLeft,
    ChevronRight,
    Zap,
  } from 'lucide-react';
  import { Button } from '@/components/ui/button';
  
  interface NavItem {
    icon: React.ElementType;
    label: string;
    active?: boolean;
  }
  
  const navItems: { icon: any, label: string }[] = [
    { icon: LayoutDashboard, label: 'Dashboard' },
    { icon: Settings, label: 'Settings' },
  ];
  
  interface SidebarProps {
    currentView: string;
    onNavigate: (view: string) => void;
  }
  
  export const Sidebar = ({ currentView, onNavigate }: SidebarProps) => {
    const [collapsed, setCollapsed] = useState(false);
  
    return (
      <aside
        className={cn(
          'flex flex-col h-screen bg-sidebar border-r border-sidebar-border transition-all duration-300',
          collapsed ? 'w-20' : 'w-64'
        )}
      >
        <div
          onClick={() => onNavigate('Dashboard')}
          className="flex items-center gap-3 p-6 border-b border-sidebar-border cursor-pointer group"
        >
          <div className="flex items-center justify-center transition-transform group-hover:scale-110 duration-300 w-10 h-10 text-primary">
            <img
              src="/logo-icon.svg"
              alt="Nendo"
              className="w-full h-full"
            />
          </div>
          {!collapsed && (
            <div className="flex flex-col">
              <span className="font-bold text-foreground tracking-tighter text-lg leading-none">NENDO</span>
              <span className="text-[10px] text-primary font-bold uppercase tracking-widest opacity-80">Intelligence</span>
            </div>
          )}
        </div>
  
        {/* Navigation */}
        <nav className="flex-1 p-4 space-y-2">
          {navItems.map((item) => (
            <button
              key={item.label}
              onClick={() => onNavigate(item.label)}
              className={cn(
                'flex items-center gap-3 w-full px-4 py-3 rounded-lg transition-all duration-200',
                currentView === item.label
                  ? 'bg-sidebar-accent text-sidebar-primary'
                  : 'text-sidebar-foreground hover:bg-sidebar-accent hover:text-foreground'
              )}
            >
              <item.icon className={cn('w-5 h-5', currentView === item.label && 'text-primary')} />
              {!collapsed && <span className="font-medium">{item.label}</span>}
            </button>
          ))}
        </nav>
  
        {/* Collapse Toggle Only */}
        <div className="p-4 border-t border-sidebar-border">
          <Button
            variant="ghost"
            size="icon"
            onClick={() => setCollapsed(!collapsed)}
            className="w-full text-muted-foreground hover:text-foreground"
          >
            {collapsed ? (
              <ChevronRight className="w-5 h-5" />
            ) : (
              <ChevronLeft className="w-5 h-5" />
            )}
          </Button>
        </div>
      </aside>
    );
  };
  </file>
  <file path="frontend\vite.config.ts">
  import { defineConfig } from "vite";
  import react from "@vitejs/plugin-react-swc";
  import path from "path";
  
  // https://vitejs.dev/config/
  export default defineConfig(({ mode }) => ({
    server: {
      host: "::",
      port: 8081,
      hmr: {
        overlay: false,
      },
    },
    plugins: [react()],
    resolve: {
      alias: {
        "@": path.resolve(__dirname, "./src"),
      },
    },
  }));
  
  </file>
  <file path="frontend\README.md">
  # Nendo TikTok Bot - Frontend
  
  This is the curated React frontend for the Nendo TikTok Intelligence Platform.
  
  ## üöÄ Getting Started
  
  ### Prerequisites
  - Node.js (v18+)
  - npm or bun
  
  ### Installation
  1. Navigate to the frontend directory:
     ```bash
     cd frontend
     ```
  2. Install dependencies:
     ```bash
     npm install
     ```
  3. Start the development server:
     ```bash
     npm run dev
     ```
  
  ## üõ†Ô∏è Tech Stack
  - **Vite**: Next-generation frontend tooling
  - **TypeScript**: Static typing for JavaScript
  - **React**: UI library
  - **Shadcn UI**: Beautifully designed components
  - **Tailwind CSS**: Utility-first CSS framework
  - **Recharts**: Composant charting library
  - **Lucide React**: Clean icons
  
  ## üìä Features
  - Live scraping controls
  - Interactive sentiment pie charts
  - Engagement timeline with activity peaks
  - Detailed content log with direct TikTok links
  - Filterable video gallery
  
  ## üåê Deployment
  This project is configured for deployment on **Vercel**. Simply connect your GitHub repository and ensure the build settings are:
  - **Build Command**: `npm run build`
  - **Output Directory**: `dist`
  - **Root Directory**: `frontend`
  
  </file>
  <file path="frontend\package.json">
  {
    "name": "vite_react_shadcn_ts",
    "private": true,
    "version": "0.0.0",
    "type": "module",
    "scripts": {
      "dev": "vite",
      "build": "vite build",
      "build:dev": "vite build --mode development",
      "lint": "eslint .",
      "preview": "vite preview",
      "test": "vitest run",
      "test:watch": "vitest"
    },
    "dependencies": {
      "@hookform/resolvers": "^3.10.0",
      "@radix-ui/react-accordion": "^1.2.11",
      "@radix-ui/react-alert-dialog": "^1.1.14",
      "@radix-ui/react-aspect-ratio": "^1.1.7",
      "@radix-ui/react-avatar": "^1.1.10",
      "@radix-ui/react-checkbox": "^1.3.2",
      "@radix-ui/react-collapsible": "^1.1.11",
      "@radix-ui/react-context-menu": "^2.2.15",
      "@radix-ui/react-dialog": "^1.1.14",
      "@radix-ui/react-dropdown-menu": "^2.1.15",
      "@radix-ui/react-hover-card": "^1.1.14",
      "@radix-ui/react-label": "^2.1.7",
      "@radix-ui/react-menubar": "^1.1.15",
      "@radix-ui/react-navigation-menu": "^1.2.13",
      "@radix-ui/react-popover": "^1.1.14",
      "@radix-ui/react-progress": "^1.1.7",
      "@radix-ui/react-radio-group": "^1.3.7",
      "@radix-ui/react-scroll-area": "^1.2.9",
      "@radix-ui/react-select": "^2.2.5",
      "@radix-ui/react-separator": "^1.1.7",
      "@radix-ui/react-slider": "^1.3.5",
      "@radix-ui/react-slot": "^1.2.3",
      "@radix-ui/react-switch": "^1.2.5",
      "@radix-ui/react-tabs": "^1.1.12",
      "@radix-ui/react-toast": "^1.2.14",
      "@radix-ui/react-toggle": "^1.1.9",
      "@radix-ui/react-toggle-group": "^1.1.10",
      "@radix-ui/react-tooltip": "^1.2.7",
      "@tanstack/react-query": "^5.83.0",
      "class-variance-authority": "^0.7.1",
      "clsx": "^2.1.1",
      "cmdk": "^1.1.1",
      "date-fns": "^3.6.0",
      "embla-carousel-react": "^8.6.0",
      "input-otp": "^1.4.2",
      "lucide-react": "^0.462.0",
      "next-themes": "^0.3.0",
      "react": "^18.3.1",
      "react-day-picker": "^8.10.1",
      "react-dom": "^18.3.1",
      "react-hook-form": "^7.61.1",
      "react-resizable-panels": "^2.1.9",
      "react-router-dom": "^6.30.1",
      "recharts": "^2.15.4",
      "sonner": "^1.7.4",
      "tailwind-merge": "^2.6.0",
      "tailwindcss-animate": "^1.0.7",
      "vaul": "^0.9.9",
      "zod": "^3.25.76"
    },
    "devDependencies": {
      "@eslint/js": "^9.32.0",
      "@testing-library/jest-dom": "^6.6.0",
      "@testing-library/react": "^16.0.0",
      "@tailwindcss/typography": "^0.5.16",
      "@types/node": "^22.16.5",
      "@types/react": "^18.3.23",
      "@types/react-dom": "^18.3.7",
      "@vitejs/plugin-react-swc": "^3.11.0",
      "autoprefixer": "^10.4.21",
      "eslint": "^9.32.0",
      "eslint-plugin-react-hooks": "^5.2.0",
      "eslint-plugin-react-refresh": "^0.4.20",
      "globals": "^15.15.0",
      "jsdom": "^20.0.3",
      "postcss": "^8.5.6",
      "tailwindcss": "^3.4.17",
      "typescript": "^5.8.3",
      "typescript-eslint": "^8.38.0",
      "vite": "^5.4.19",
      "vitest": "^3.2.4"
    }
  }
  </file>
  <file path=".gitignore">
  # Python
  __pycache__/
  *.py[cod]
  *$py.class
  *.so
  .Python
  build/
  develop-eggs/
  dist/
  downloads/
  eggs/
  .eggs/
  lib/
  lib64/
  parts/
  sdist/
  var/
  wheels/
  share/python-wheels/
  *.egg-info/
  .installed.cfg
  *.egg
  MANIFEST
  
  # Virtual Environment
  venv/
  env/
  ENV/
  .env
  
  # Secrets
  credentials.json
  secrets.json
  client_secret.json
  
  # Streamlit
  .streamlit/
  
  # Node
  node_modules/
  frontend/node_modules/
  frontend/dist/
  
  # Local Data
  data_cache.json
  config.json
  
  # OS
  .DS_Store
  Thumbs.db
  
  </file>
  <file path="scraper.py">
  """
  TikTok scraper module using Apify Actor (clockworks/tiktok-scraper)
  """
  from apify_client import ApifyClient
  import asyncio
  from datetime import datetime
  import os
  import re
  
  
  class TikTokScraper:
      def __init__(self, api_token=None):
          """
          Initialize Apify scraper
          
          Args:
              api_token: Apify API token. If None, looks for APIFY_API_TOKEN env var.
          """
          self.token = api_token or os.getenv('APIFY_API_TOKEN')
          if not self.token:
              print("‚ö†Ô∏è No Apify API token provided. Scraper will fail unless token is passed.")
              self.client = None
          else:
              self.client = ApifyClient(self.token)
          
          # Using clockworks/tiktok-scraper as it is reliable
          self.actor_id = "clockworks/tiktok-scraper"
  
      async def initialize(self):
          """Check connection - lightweight for Apify"""
          if not self.client and self.token:
              self.client = ApifyClient(self.token)
          return True if self.client else False
      
      def extract_hashtags(self, caption):
          """Helper to extract hashtags from caption"""
          if not caption: return ""
          return ', '.join(re.findall(r'#(\w+)', caption))
  
      def extract_mentions(self, caption):
          """Helper to extract mentions from caption"""
          if not caption: return ""
          return ', '.join(re.findall(r'@(\w+)', caption))
  
      def _map_result(self, item, extract_comments=False):
          """Map Apify result to our app's data structure"""
          try:
              # Video Date
              video_date = datetime.now()
              if 'createTime' in item:
                  try: video_date = datetime.fromtimestamp(item['createTime'])
                  except: pass
              elif 'createTimeISO' in item:
                  try: video_date = datetime.fromisoformat(item['createTimeISO'].replace('Z', '+00:00'))
                  except: pass
  
              video_data = {
                  'video_id': item.get('id', item.get('videoMeta', {}).get('id', '')),
                  'video_url': item.get('webVideoUrl', ''),
                  'caption': item.get('text', ''),
                  'author': item.get('authorMeta', {}).get('name', ''),
                  'likes': item.get('diggCount', 0),
                  'comments': item.get('commentCount', 0),
                  'shares': item.get('shareCount', 0),
                  'saves': item.get('collectCount', 0),
                  'views': item.get('playCount', 0),
                  'publish_date': video_date.strftime('%Y-%m-%d %H:%M:%S'),
                  'hashtags': ', '.join([t.get('name', '') for t in item.get('hashtags', [])]) or self.extract_hashtags(item.get('text', '')),
                  'mentions': self.extract_mentions(item.get('text', '')),
                  'thumbnail_url': item.get('coverUrl', item.get('videoMeta', {}).get('coverUrl', ''))
              }
  
              if extract_comments:
                  comments_list = []
                  # clockworks/tiktok-scraper usually returns comments in 'comments' list if requested
                  raw_comments = item.get('comments', [])
                  for c in raw_comments:
                      try:
                          c_date = datetime.now()
                          if 'createTime' in c:
                               try: c_date = datetime.fromtimestamp(c['createTime'])
                               except: pass
                          
                          comments_list.append({
                              'comment_id': c.get('id', ''),
                              'video_id': video_data['video_id'],
                              'text': c.get('text', ''),
                              'author': c.get('authorUniqueId', ''),
                              'date': c_date.strftime('%Y-%m-%d %H:%M:%S'),
                              'likes': c.get('diggCount', 0)
                          })
                      except: continue
                  video_data['scraped_comments'] = comments_list
  
              return video_data
          except Exception as e:
              print(f"Error mapping item: {e}")
              return None
  
      async def _run_actor(self, run_input, limit=None, since_date=None, comments_per_video=0):
          """Generic actor runner with limits"""
          if not self.client:
              print("‚ùå Apify Client not initialized. Missing API Token.")
              return []
          
          print(f"üöÄ Starting Apify Actor: {self.actor_id} with input: {run_input}")
          
          results = []
          try:
              run = self.client.actor(self.actor_id).call(run_input=run_input)
              
              dataset_id = run.get('defaultDatasetId')
              if not dataset_id:
                  return []
              
              print(f"‚úÖ Run finished. Fetching results from dataset {dataset_id}...")
              
              dataset_items = self.client.dataset(dataset_id).iterate_items()
              
              count = 0
              for item in dataset_items:
                  # Map result (extract comments if requested)
                  mapped = self._map_result(item, extract_comments=(comments_per_video > 0))
                  if not mapped: continue
                  
                  # Check limits
                  if since_date:
                      item_date = datetime.strptime(mapped['publish_date'], '%Y-%m-%d %H:%M:%S')
                      if item_date < since_date:
                          continue 
                  
                  results.append(mapped)
                  count += 1
                  
                  if limit and count >= limit:
                      break
              
              return results
              
          except Exception as e:
              print(f"‚ùå Error running Apify actor: {e}")
              return []
  
      async def scrape_hashtag(self, hashtags, count=None, since_date=None, comments_per_video=0):
          if isinstance(hashtags, str):
              hashtags = [h.strip() for h in hashtags.split(',')]
              
          limit = count if count else 100
          run_input = {
              "hashtags": hashtags,
              "resultsPerPage": limit,
              "shouldDownloadVideos": False,
              "commentsPerVideo": comments_per_video
          }
          return await self._run_actor(run_input, limit, since_date, comments_per_video)
  
      async def scrape_user(self, usernames, count=None, since_date=None, comments_per_video=0):
          if isinstance(usernames, str):
              usernames = [u.strip() for u in usernames.split(',')]
              
          limit = count if count else 100
          run_input = {
              "profiles": usernames,
              "resultsPerPage": limit,
              "shouldDownloadVideos": False,
              "commentsPerVideo": comments_per_video
          }
          return await self._run_actor(run_input, limit, since_date, comments_per_video)
  
      async def scrape_search(self, queries, count=None, since_date=None, comments_per_video=0):
          if isinstance(queries, str):
              queries = [q.strip() for q in queries.split(',')]
          
          limit = count if count else 100
          run_input = {
              "searchQueries": queries,
              "resultsPerPage": limit,
              "shouldDownloadVideos": False,
              "commentsPerVideo": comments_per_video
          }
          return await self._run_actor(run_input, limit, since_date, comments_per_video)
  
      async def close(self):
          pass
  
  
  # Synchronous wrappers
  def scrape_hashtag_sync(hashtags, count=None, since_date=None, api_token=None, comments_per_video=0):
      scraper = TikTokScraper(api_token)
      return asyncio.run(scraper.scrape_hashtag(hashtags, count, since_date, comments_per_video))
  
  def scrape_user_sync(usernames, count=None, since_date=None, api_token=None, comments_per_video=0):
      scraper = TikTokScraper(api_token)
      return asyncio.run(scraper.scrape_user(usernames, count, since_date, comments_per_video))
  
  def scrape_search_sync(queries, count=None, since_date=None, api_token=None, comments_per_video=0):
      scraper = TikTokScraper(api_token)
      return asyncio.run(scraper.scrape_search(queries, count, since_date, comments_per_video))
  
  </file>
  <file path="sheets.py">
  """
  Google Sheets integration module for storing TikTok data
  """
  import gspread
  from oauth2client.service_account import ServiceAccountCredentials
  import pandas as pd
  from datetime import datetime
  import os
  
  
  class SheetsManager:
      def __init__(self, credentials_file='credentials.json', sheet_name='TikTok Analytics', sheet_url='https://docs.google.com/spreadsheets/d/1dirtUrGwr-r3sfpocamL1N8xkwJEk1xn8lcMfrU_bI0/edit?usp=sharing'):
          """
          Initialize Google Sheets connection
          
          Args:
              credentials_file: Path to Google service account credentials JSON
              sheet_name: Name of the Google Sheet to use (fallback)
              sheet_url: Direct URL to the Google Sheet (preferred)
          """
          self.sheet_name = sheet_name
          self.sheet_url = sheet_url
          self.credentials_file = credentials_file
          self.client = None
          self.sheet = None
          self.worksheet = None
          
      def connect(self):
          """Establish connection to Google Sheets"""
          try:
              scope = [
                  'https://spreadsheets.google.com/feeds',
                  'https://www.googleapis.com/auth/drive'
              ]
              
              if not os.path.exists(self.credentials_file):
                  print(f"‚ö†Ô∏è Credentials file not found: {self.credentials_file}")
                  return False
              
              creds = ServiceAccountCredentials.from_json_keyfile_name(
                  self.credentials_file, scope
              )
              self.client = gspread.authorize(creds)
              
              # Try to open by URL first (most reliable), then by name
              try:
                  if self.sheet_url:
                      print(f"üîó Opening sheet by URL: {self.sheet_url}")
                      self.sheet = self.client.open_by_url(self.sheet_url)
                  else:
                      self.sheet = self.client.open(self.sheet_name)
              except gspread.SpreadsheetNotFound:
                  if self.sheet_url:
                       print(f"‚ùå Could not find sheet with URL. Checking by name...")
                  
                  try:
                      self.sheet = self.client.open(self.sheet_name)
                  except gspread.SpreadsheetNotFound:
                      print(f"Creating new spreadsheet: {self.sheet_name}")
                      self.sheet = self.client.create(self.sheet_name)
                      self.sheet.share('', perm_type='anyone', role='reader')
              
              # Get or create worksheet (Videos)
              try:
                  self.worksheet = self.sheet.sheet1
                  if self.worksheet.title.lower() != 'data':
                       try: self.worksheet.update_title('Videos')
                       except: pass
              except:
                  self.worksheet = self.sheet.add_worksheet(title="Videos", rows="1000", cols="20")
              
              # Initialize headers if empty
              if not self.worksheet.row_values(1):
                  headers = [
                      'timestamp', 'video_id', 'video_url', 'caption', 'author',
                      'likes', 'comments', 'shares', 'saves', 'views', 'publish_date',
                      'hashtags', 'mentions', 'thumbnail_url'
                  ]
                  self.worksheet.append_row(headers)
  
              # Get or create Comments worksheet
              try:
                  self.comments_sheet = self.sheet.worksheet("Comments")
              except:
                  self.comments_sheet = self.sheet.add_worksheet(title="Comments", rows="1000", cols="8")
                  # Headers for comments
                  self.comments_sheet.append_row([
                      'scraped_at', 'video_id', 'comment_id', 'author', 
                      'text', 'likes', 'date'
                  ])
              
              print(f"‚úÖ Connected to Google Sheets: {self.sheet_name}")
              return True
              
          except Exception as e:
              print(f"‚ùå Error connecting to Google Sheets: {e}")
              return False
      
      def get_existing_video_ids(self):
          """Get list of existing video IDs to avoid duplicates"""
          try:
              if not self.worksheet:
                  return set()
              video_ids = self.worksheet.col_values(2)[1:]  # Skip header
              return set(video_ids)
          except Exception as e:
              print(f"Error getting existing video IDs: {e}")
              return set()
      
      def append_comments(self, comments_list):
          """
          Append comments to the separate Comments sheet
          """
          if not self.comments_sheet or not comments_list:
              return 0
          
          try:
              # We don't check for duplicate comments strictly here to save API calls/time, 
              # but ideally we would. For now, just append.
              # Convert list of dicts to list of lists
              rows = []
              timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
              
              for c in comments_list:
                  rows.append([
                      timestamp,
                      c.get('video_id', ''),
                      c.get('comment_id', ''),
                      c.get('author', ''),
                      c.get('text', ''),
                      c.get('likes', 0),
                      c.get('date', '')
                  ])
              
              if rows:
                  self.comments_sheet.append_rows(rows)
                  print(f"‚úÖ Added {len(rows)} comments to Sheets")
              
              return len(rows)
          except Exception as e:
              print(f"‚ùå Error appending comments: {e}")
              return 0
  
      def append_data(self, data_list):
          """
          Append new TikTok data to sheet, avoiding duplicates
          
          Args:
              data_list: List of dictionaries containing TikTok video data
          
          Returns:
              Number of new rows added
          """
          if not self.worksheet:
              print("Not connected to Google Sheets")
              return 0
          
          try:
              existing_ids = self.get_existing_video_ids()
              new_rows = 0
              
              for data in data_list:
                  video_id = str(data.get('video_id', ''))
                  
                  # Skip if already exists
                  if video_id in existing_ids:
                      continue
                  
                  row = [
                      datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                      video_id,
                      data.get('video_url', ''),
                      data.get('caption', ''),
                      data.get('author', ''),
                      data.get('likes', 0),
                      data.get('comments', 0),
                      data.get('shares', 0),
                      data.get('saves', 0),
                      data.get('views', 0),
                      data.get('publish_date', ''),
                      data.get('hashtags', ''),
                      data.get('mentions', ''),
                      data.get('thumbnail_url', '')
                  ]
                  
                  self.worksheet.append_row(row)
                  existing_ids.add(video_id)
                  new_rows += 1
              
              print(f"‚úÖ Added {new_rows} new rows to Google Sheets")
              return new_rows
              
          except Exception as e:
              print(f"‚ùå Error appending data: {e}")
              return 0
      
      def get_all_data(self):
          """
          Retrieve all data from sheet as pandas DataFrame
          
          Returns:
              pandas DataFrame with all data
          """
          try:
              if not self.worksheet:
                  return pd.DataFrame()
              
              data = self.worksheet.get_all_records()
              df = pd.DataFrame(data)
              
              # Convert numeric columns
              numeric_cols = ['likes', 'comments', 'shares', 'saves', 'views']
              for col in numeric_cols:
                  if col in df.columns:
                      df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
              
              # Convert date columns
              if 'publish_date' in df.columns:
                  df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce')
              if 'timestamp' in df.columns:
                  df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
              
              return df
              
          except Exception as e:
              print(f"Error retrieving data: {e}")
              return pd.DataFrame()
      def get_all_comments(self):
          """
          Retrieve all comment data from the Comments sheet as pandas DataFrame
          
          Returns:
              pandas DataFrame with all comments
          """
          try:
              if not self.comments_sheet:
                  return pd.DataFrame()
              
              data = self.comments_sheet.get_all_records()
              df = pd.DataFrame(data)
              
              # Convert numeric columns
              if 'likes' in df.columns:
                  df['likes'] = pd.to_numeric(df['likes'], errors='coerce').fillna(0)
              
              # Convert date columns
              if 'date' in df.columns:
                  df['date'] = pd.to_datetime(df['date'], errors='coerce')
              if 'scraped_at' in df.columns:
                  df['scraped_at'] = pd.to_datetime(df['scraped_at'], errors='coerce')
              
              return df
              
          except Exception as e:
              print(f"Error retrieving comments: {e}")
              return pd.DataFrame()
      
      def get_sheet_url(self):
          """Get the URL of the Google Sheet"""
          if self.sheet:
              return self.sheet.url
          return None
  
  </file>
  <file path="app.py">
  """
  TikTok Social Listening & Analytics Dashboard
  Main Streamlit application
  """
  import streamlit as st
  import plotly.express as px
  import plotly.graph_objects as go
  from wordcloud import WordCloud
  import matplotlib.pyplot as plt
  import pandas as pd
  from datetime import datetime, timedelta
  import os
  
  from scraper import scrape_hashtag_sync, scrape_user_sync
  from analysis import TikTokAnalyzer
  from sheets import SheetsManager
  
  
  # Page configuration
  st.set_page_config(
      page_title="TikTok Analytics Dashboard",
      page_icon="üìä",
      layout="wide",
      initial_sidebar_state="expanded"
  )
  
  # Standard Streamlit styling (Plain)
  st.markdown("""
  <style>
      .main-header {
          font-size: 2.5rem;
          font-weight: 700;
          margin-bottom: 0.5rem;
      }
  </style>
  """, unsafe_allow_html=True)
  
  
  
  # Initialize session state
  if 'data_loaded' not in st.session_state:
      st.session_state.data_loaded = False
  if 'df' not in st.session_state:
      st.session_state.df = pd.DataFrame()
  if 'df_comments' not in st.session_state:
      st.session_state.df_comments = pd.DataFrame()
  if 'df_comments_filtered' not in st.session_state:
      st.session_state.df_comments_filtered = pd.DataFrame()
  
  
  @st.cache_resource
  def get_sheets_manager():
      """Initialize and cache Google Sheets manager"""
      manager = SheetsManager()
      manager.connect()
      return manager
  
  
  @st.cache_resource
  def get_analyzer():
      """Initialize and cache analyzer"""
      return TikTokAnalyzer()
  
  
  def create_wordcloud(text_data):
      """Generate word cloud from text data"""
      if not text_data:
          return None
      
      wordcloud = WordCloud(
          width=800,
          height=400,
          background_color='white',
          colormap='viridis',
          max_words=50
      ).generate(text_data)
      
      fig, ax = plt.subplots(figsize=(10, 5))
      ax.imshow(wordcloud, interpolation='bilinear')
      ax.axis('off')
      plt.tight_layout(pad=0)
      
      return fig
  
  
  
   def main():
       # Header
       st.markdown('<h1 class="main-header">üìä TikTok Social Listening Dashboard</h1>', unsafe_allow_html=True)
      st.markdown("Track hashtags, analyze sentiment, and discover insights from TikTok content")
      
      # Sidebar
      st.sidebar.title("üéØ Filters & Controls")
      
      # Connection Status
      if not os.path.exists("credentials.json"):
          st.sidebar.error("‚ùå Google Sheets: Disconnected")
          st.sidebar.caption("Missing `credentials.json`. Data will be temporary.")
      else:
          st.sidebar.success("‚úÖ Google Sheets: File Found")
      
      # Scraping controls
      st.sidebar.header("Data Collection")
      
      # Apify Token Input
      apify_token = st.sidebar.text_input("Apify API Token", type="password", help="Get your token from https://console.apify.com/account/integrations")
      if not apify_token:
          st.sidebar.warning("‚ö†Ô∏è Apify Token required")
      
      scrape_type = st.sidebar.radio("Scrape by:", ["Hashtag", "Username", "Keyword/Search"])
      
      if scrape_type == "Hashtag":
          search_input = st.sidebar.text_input("Enter hashtag (without #)", placeholder="viral")
      elif scrape_type == "Username":
          search_input = st.sidebar.text_input("Enter username (without @)", placeholder="username")
      else:
          search_input = st.sidebar.text_input("Enter keyword (treated as hashtag)", placeholder="tiktok analytics")
      
      # Date limit for scraping
      use_date_limit = st.sidebar.checkbox("Limit by Date (Stop if older)")
      since_date = None
      if use_date_limit:
          since_date_input = st.sidebar.date_input("Scrape videos since:", 
                                                  value=datetime.now().date() - timedelta(days=7),
                                                  max_value=datetime.now().date())
          since_date = datetime.combine(since_date_input, datetime.min.time())
      
      # Count limit
      no_limit = st.sidebar.checkbox("No Video Limit")
      if no_limit:
          video_count = None
          st.sidebar.caption("‚ö†Ô∏è Might consume many Apify credits.")
      else:
          video_count = st.sidebar.number_input("Max videos to scrape", min_value=10, max_value=10000, value=50, step=10)
      
      # Comments Scraping (Now for all modes)
      st.sidebar.subheader("üí¨ Comments")
      scrape_comments = st.sidebar.checkbox("Scrape Comments for each video")
      comments_limit = 0
      if scrape_comments:
          comments_limit = st.sidebar.number_input("Max comments per video", min_value=10, max_value=2000, value=50)
      
      if st.sidebar.button("üîÑ Scrape New Data", use_container_width=True):
          if not apify_token:
              st.sidebar.error("Please enter your Apify API Token first!")
          elif search_input:
              with st.spinner(f"Running Apify Actor for '{search_input}'..."):
                  try:
                      # Determine scraper function
                      results = []
                      if scrape_type == "Hashtag":
                          results = scrape_hashtag_sync(search_input, video_count, since_date, apify_token, comments_limit if scrape_comments else 0)
                      elif scrape_type == "Username":
                          # Pass comment limits
                          results = scrape_user_sync(search_input, video_count, since_date, apify_token, comments_limit if scrape_comments else 0)
                      else:
                          from scraper import scrape_search_sync
                          results = scrape_search_sync(search_input, video_count, since_date, apify_token, comments_limit if scrape_comments else 0)
                      
                      if results:
                          # Extract comments if any
                          all_comments = []
                          for video in results:
                              if 'scraped_comments' in video:
                                  all_comments.extend(video['scraped_comments'])
                                  # Remove from video dict before saving to main sheet to keep it clean
                                  del video['scraped_comments']
  
                          # Save to Google Sheets
                          sheets = get_sheets_manager()
                          if sheets.worksheet:
                              new_rows = sheets.append_data(results)
                              msg = f"‚úÖ Scraped {len(results)} videos"
                              
                              if all_comments:
                                  new_comments = sheets.append_comments(all_comments)
                                  msg += f" and {new_comments} comments"
                              
                              st.sidebar.success(msg)
                          else:
                              st.sidebar.warning("‚ö†Ô∏è Google Sheets not connected. Data not saved.")
                              if st.session_state.df.empty:
                                  st.session_state.df = pd.DataFrame(results)
                              else:
                                  st.session_state.df = pd.concat([st.session_state.df, pd.DataFrame(results)], ignore_index=True)
                          
                          # Update session state immediately so UI refreshes without manual load
                          new_df = pd.DataFrame(results)
                          analyzer = get_analyzer()
                          new_df = analyzer.calculate_engagement_rate(new_df)
                          new_df = analyzer.add_sentiment_analysis(new_df, method='vader')
                          
                          if st.session_state.df.empty:
                              st.session_state.df = new_df
                          else:
                              st.session_state.df = pd.concat([st.session_state.df, new_df], ignore_index=True).drop_duplicates('video_id')
                          
                          if all_comments:
                              new_c_df = pd.DataFrame(all_comments)
                              new_c_df = analyzer.add_sentiment_analysis(new_c_df, method='vader', text_column='text')
                              if st.session_state.df_comments.empty:
                                  st.session_state.df_comments = new_c_df
                              else:
                                  st.session_state.df_comments = pd.concat([st.session_state.df_comments, new_c_df], ignore_index=True).drop_duplicates('comment_id')
                              st.session_state.df_comments_filtered = st.session_state.df_comments
                          
                          st.session_state.data_loaded = True
                          
                      else:
                          st.sidebar.error("No data found or Apify run failed. Check your token and credits.")
                          
                  except Exception as e:
                      st.sidebar.error(f"Error scraping: {str(e)}")
          else:
              st.sidebar.warning("Please enter a search term")
      
      st.sidebar.divider()
      
      # Load data from Google Sheets
      if st.sidebar.button("üì• Load Data from Sheets", use_container_width=True):
          with st.spinner("Loading data from Google Sheets..."):
              sheets = get_sheets_manager()
              df = sheets.get_all_data()
              
              if not df.empty:
                  # Add analytics
                  analyzer = get_analyzer()
                  df = analyzer.calculate_engagement_rate(df)
                  df = analyzer.add_sentiment_analysis(df, method='vader')
                  
                  st.session_state.df = df
                  
                  # Load comments
                  df_comments = sheets.get_all_comments()
                  if not df_comments.empty:
                      df_comments = analyzer.add_sentiment_analysis(df_comments, method='vader', text_column='text')
                  st.session_state.df_comments = df_comments
                  st.session_state.df_comments_filtered = df_comments # Initialize
                  
                  st.session_state.data_loaded = True
                  st.sidebar.success(f"‚úÖ Loaded {len(df)} videos and {len(df_comments)} comments")
              else:
                  st.sidebar.warning("No data found in Google Sheets")
      
      # Date range filter
      if st.session_state.data_loaded and not st.session_state.df.empty:
          st.sidebar.header("üìÖ Date Range")
          
          df = st.session_state.df
          
          if 'publish_date' in df.columns:
              # Ensure publish_date is datetime
              df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce')
              
              # Drop rows with invalid dates if any
              df = df.dropna(subset=['publish_date'])
              
              if not df.empty:
                  min_date = df['publish_date'].min().date()
                  max_date = df['publish_date'].max().date()
                  
                  date_range = st.sidebar.date_input(
                      "Select date range",
                      value=(min_date, max_date),
                      min_value=min_date,
                      max_value=max_date
                  )
              
              if len(date_range) == 2:
                  mask = (df['publish_date'].dt.date >= date_range[0]) & (df['publish_date'].dt.date <= date_range[1])
                  df = df[mask]
                  
                  # Also filter comments if they exist
                  if not st.session_state.df_comments.empty:
                      df_c = st.session_state.df_comments
                      if 'date' in df_c.columns:
                          mask_c = (df_c['date'].dt.date >= date_range[0]) & (df_c['date'].dt.date <= date_range[1])
                          st.session_state.df_comments_filtered = df_c[mask_c]
                      else:
                          st.session_state.df_comments_filtered = df_c
                  else:
                      st.session_state.df_comments_filtered = pd.DataFrame()
      
      # Main content
      if st.session_state.data_loaded and not st.session_state.df.empty:
          df = st.session_state.df
          analyzer = get_analyzer()
          
          # Ensure analytics are calculated (for both fresh scrapes and loaded data)
          if 'engagement_rate' not in df.columns:
              df = analyzer.calculate_engagement_rate(df)
          if 'sentiment' not in df.columns or 'sentiment_score' not in df.columns:
              df = analyzer.add_sentiment_analysis(df, method='vader')
          
          # Update session state with enriched data
          st.session_state.df = df
          
          # Key metrics
          col1, col2, col3, col4, col5 = st.columns(5)
          
          with col1:
              st.metric("Total Videos", f"{len(df):,}")
          with col2:
              st.metric("Total Views", f"{df['views'].sum():,.0f}")
          with col3:
              st.metric("Total Likes", f"{df['likes'].sum():,.0f}")
          with col4:
              st.metric("Total Shares", f"{df['shares'].sum():,.0f}")
          with col5:
              # Check if saves exist in the dataset
              saves_sum = df['saves'].sum() if 'saves' in df.columns else 0
              st.metric("Total Saves", f"{saves_sum:,.0f}")
          
          # New row for averages
          acol1, acol2, acol3 = st.columns(3)
          with acol1:
              st.metric("Avg Engagement", f"{df['engagement_rate'].mean():.2f}%")
          with acol2:
              st.metric("Total Comments", f"{df['comments'].sum():,.0f}")
          with acol3:
              # Comment data count
              c_count = len(st.session_state.df_comments_filtered) if not st.session_state.df_comments_filtered.empty else 0
              st.metric("Scraped Comments", f"{c_count:,}")
          
          st.divider()
          
          # Top Content Gallery
          st.subheader("üî• Top Trending Videos")
          top_videos = df.sort_values('views', ascending=False).head(4)
          
          vid_cols = st.columns(4)
          for idx, (_, row) in enumerate(top_videos.iterrows()):
              with vid_cols[idx]:
                  st.markdown(f"**@{row['author']}**")
                  # Clean caption for display
                  caption_preview = (row['caption'][:50] + '...') if len(row['caption']) > 50 else row['caption']
                  st.caption(f"{caption_preview}")
                  
                  # Metric Badge
                  st.markdown(f"""
                  <div style="background: rgba(255,255,255,0.1); padding: 5px; border-radius: 5px; font-size: 0.8em; margin-bottom: 5px;">
                      üëÅÔ∏è {row['views']:,} | ‚ù§Ô∏è {row['likes']:,} | üí¨ {row['comments']:,} | Mood: {row['sentiment']}
                  </div>
                  """, unsafe_allow_html=True)
                  
                  st.markdown(f"[üé• Watch Video]({row['video_url']})")
  
          st.divider()
  
          # Advanced Interactive Charts
          col1, col2 = st.columns([2, 1])
          
          with col1:
              st.subheader("üó∫Ô∏è Engagement Landscape")
              # Bubble Chart: X=Time, Y=Views, Size=Likes+Comments, Color=Sentiment
              
              # Normalize size for bubbles
              df['total_engagement'] = df['likes'] + df['comments'] + df['shares']
              
              fig = px.scatter(
                  df,
                  x='publish_date',
                  y='views',
                  size='total_engagement',
                  color='sentiment',
                  hover_data=['author', 'caption', 'video_url'],
                  title='Content Performance Timeline (Bubble Size = Engagement)',
                  color_discrete_map={'positive': '#00CC96', 'neutral': '#AB63FA', 'negative': '#EF553B'},
                  render_mode='webgl'
              )
              
              fig.update_layout(
                  plot_bgcolor='rgba(0,0,0,0)',
                  paper_bgcolor='rgba(0,0,0,0)',
                  xaxis=dict(showgrid=False, title="Publish Date"),
                  yaxis=dict(showgrid=True, title="Views"),
                  hovermode='closest'
              )
              st.plotly_chart(fig, use_container_width=True)
              
          with col2:
              st.subheader("üéØ Sentiment Distribution")
              sentiment_counts = df['sentiment'].value_counts().reset_index()
              sentiment_counts.columns = ['sentiment', 'count']
              
              fig_pie = px.pie(
                  sentiment_counts, 
                  values='count', 
                  names='sentiment',
                  color='sentiment',
                  color_discrete_map={'positive': '#00CC96', 'neutral': '#AB63FA', 'negative': '#EF553B'},
                  hole=0.4
              )
              fig_pie.update_layout(
                  paper_bgcolor='rgba(0,0,0,0)',
                  legend=dict(orientation="h", yanchor="bottom", y=-0.1, xanchor="center", x=0.5)
              )
              st.plotly_chart(fig_pie, use_container_width=True)
  
  
          st.subheader("‚ö° Engagement Breakdown")
          col1, col2 = st.columns([2, 1])
          with col1:
              # Stacked Bar of interactions
              time_agg = analyzer.aggregate_by_time(df, freq='D')
              if not time_agg.empty:
                  fig_bar = go.Figure(data=[
                      go.Bar(name='Likes', x=time_agg['publish_date'], y=time_agg['likes'], marker_color='#667eea'),
                      go.Bar(name='Comments', x=time_agg['publish_date'], y=time_agg['comments'], marker_color='#764ba2'),
                      go.Bar(name='Shares', x=time_agg['publish_date'], y=time_agg['shares'], marker_color='#f093fb')
                  ])
                  if 'saves' in time_agg.columns:
                       fig_bar.add_trace(go.Bar(name='Saves', x=time_agg['publish_date'], y=time_agg['saves'], marker_color='#ffcc00'))
  
                  fig_bar.update_layout(
                      barmode='stack',
                      plot_bgcolor='rgba(0,0,0,0)',
                      paper_bgcolor='rgba(0,0,0,0)',
                      legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
                  )
                  st.plotly_chart(fig_bar, use_container_width=True)
          
          with col2:
               # Additional stats or info
               st.info("The chart on the left shows daily engagement totals. You can hover over bars to see breakdown by type.")
  
          # Word cloud & Authors
          col1, col2 = st.columns(2)
          with col1:
              st.subheader("‚òÅÔ∏è Trending Keywords")
              all_captions = ' '.join(df['caption'].dropna().astype(str))
              if all_captions:
                  wordcloud_fig = create_wordcloud(all_captions)
                  if wordcloud_fig:
                      st.pyplot(wordcloud_fig, use_container_width=True)
                      
          with col2:
               st.subheader("üèÜ Influencer Leaderboard")
               author_stats = df.groupby('author').agg({
                  'video_id': 'count',
                  'views': 'sum',
                  'engagement_rate': 'mean'
              }).round(2).sort_values('views', ascending=False).head(8)
               
               # Display as a styled dataframe with bars
               st.dataframe(
                   author_stats,
                   column_config={
                       "engagement_rate": st.column_config.ProgressColumn(
                           "Engagement %",
                           format="%.2f%%",
                           min_value=0,
                           max_value=100,
                       ),
                       "views": st.column_config.NumberColumn(
                           "Total Views",
                           format="%d ‚≠ê"
                       )
                   },
                   use_container_width=True
               )
  
          # Interactive Data Table with Links
          st.subheader("üìã Detailed Content Log")
          st.markdown("Click any URL to open the video/comment.")
          
          # Prepare display copy
          display_df = df.copy()
          
          # Configure columns for clickable links
          st.dataframe(
              display_df[[
                  'publish_date', 'author', 'caption', 'video_url', 
                  'views', 'likes', 'comments', 'engagement_rate', 'sentiment'
              ]],
              column_config={
                  "video_url": st.column_config.LinkColumn(
                      "Video Link",
                      display_text="Watch Video üîó"
                  ),
                  "author": st.column_config.TextColumn("Creator"),
                  "publish_date": st.column_config.DatetimeColumn("Posted", format="D MMM YYYY, HH:mm"),
                  "views": st.column_config.NumberColumn("Views", format="%d"),
                  "engagement_rate": st.column_config.NumberColumn("Eng. Rate", format="%.2f%%"),
                  "sentiment": st.column_config.TextColumn("Mood")
              },
              hide_index=True,
              use_container_width=True
          )
  
          # COMMENTS SECTION
          st.divider()
          st.header("üí¨ Detailed Comment Analysis")
          
          df_c = st.session_state.df_comments_filtered
          
          if not df_c.empty:
              ccol1, ccol2 = st.columns([1, 2])
              
              with ccol1:
                  st.subheader("Comment Sentiment")
                  c_sentiment_counts = df_c['sentiment'].value_counts().reset_index()
                  c_sentiment_counts.columns = ['sentiment', 'count']
                  
                  fig_c_pie = px.pie(
                      c_sentiment_counts,
                      values='count',
                      names='sentiment',
                      color='sentiment',
                      color_discrete_map={'positive': '#00CC96', 'neutral': '#AB63FA', 'negative': '#EF553B'},
                      hole=0.4
                  )
                  fig_c_pie.update_layout(
                      paper_bgcolor='rgba(0,0,0,0)',
                      font_color='white',
                      showlegend=True,
                      legend=dict(orientation="h", yanchor="bottom", y=-0.1, xanchor="center", x=0.5)
                  )
                  st.plotly_chart(fig_c_pie, use_container_width=True)
                  
                  # Sentiment Score Distribution
                  fig_c_hist = px.histogram(
                      df_c, 
                      x='sentiment_score',
                      nbins=20,
                      color_discrete_sequence=['#764ba2'],
                      title="Comment Sentiment Score Distribution"
                  )
                  fig_c_hist.update_layout(paper_bgcolor='rgba(0,0,0,0)', font_color='white')
                  st.plotly_chart(fig_c_hist, use_container_width=True)
              
              with ccol2:
                  st.subheader("Trending Comment Keywords")
                  comment_text = ' '.join(df_c['text'].dropna().astype(str))
                  if comment_text:
                      c_wordcloud = create_wordcloud(comment_text)
                      if c_wordcloud:
                          st.pyplot(c_wordcloud, use_container_width=True)
                  
                  st.subheader("Top Liked Comments")
                  top_comments = df_c.sort_values('likes', ascending=False).head(10)
                  st.dataframe(
                      top_comments[['author', 'text', 'likes', 'date', 'sentiment']],
                      column_config={
                          "date": st.column_config.DatetimeColumn("Date", format="D MMM, HH:mm"),
                          "likes": st.column_config.NumberColumn("Likes", format="%d ‚ù§Ô∏è"),
                          "sentiment": st.column_config.TextColumn("Sentiment")
                      },
                      hide_index=True,
                      use_container_width=True
                  )
              
              st.subheader("Raw Comment Feed")
              st.dataframe(
                  df_c[['date', 'author', 'text', 'likes', 'sentiment', 'video_id']],
                  column_config={
                      "date": st.column_config.DatetimeColumn("Date", format="D MMM YY, HH:mm"),
                      "video_id": st.column_config.TextColumn("Video ID")
                  },
                  hide_index=True,
                  use_container_width=True
              )
          else:
              st.info("No comment data available for the selected period. Ensure you scrape with 'Scrape Comments' enabled.")
          
      else:
          # Welcome screen
          st.info("üëã Welcome! Use the sidebar to scrape TikTok data or load existing data from Google Sheets.")
          
          col1, col2, col3 = st.columns(3)
          
          with col1:
              st.markdown("### üîç Scrape Data")
              st.write("Search by hashtag or username to collect TikTok videos")
          
          with col2:
              st.markdown("### üìä Analyze")
              st.write("Get sentiment analysis, engagement metrics, and trends")
          
          with col3:
              st.markdown("### üíæ Store")
              st.write("Automatically save to Google Sheets for persistence")
      
      # Footer
      st.divider()
      sheets = get_sheets_manager()
      if sheets.get_sheet_url():
          st.info(f"üìä Data stored in: [{sheets.sheet_name}]({sheets.get_sheet_url()})")
  
  
  if __name__ == "__main__":
      main()
  
  </file>
  <file path="analysis.py">
  """
  Analytics module for TikTok data analysis
  """
  import pandas as pd
  from textblob import TextBlob
  from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
  from collections import Counter
  import re
  
  
  class TikTokAnalyzer:
      def __init__(self):
          """Initialize analyzer with sentiment tools"""
          self.vader = SentimentIntensityAnalyzer()
      
      def calculate_engagement_rate(self, df):
          """
          Calculate engagement rate for each video
          Engagement Rate = (Likes + Comments + Shares) / Views * 100
          
          Args:
              df: DataFrame with TikTok data
              
          Returns:
              DataFrame with engagement_rate column added
          """
          if df.empty:
              return df
          
          df = df.copy()
          
          # Calculate engagement rate
          df['engagement_rate'] = (
              (df['likes'] + df['comments'] + df['shares']) / 
              df['views'].replace(0, 1) * 100
          ).round(2)
          
          return df
      
      def analyze_sentiment_textblob(self, text):
          """
          Analyze sentiment using TextBlob
          
          Returns:
              Dictionary with polarity and subjectivity
          """
          if not text or pd.isna(text):
              return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'neutral'}
          
          try:
              blob = TextBlob(str(text))
              polarity = blob.sentiment.polarity
              
              if polarity > 0.1:
                  sentiment = 'positive'
              elif polarity < -0.1:
                  sentiment = 'negative'
              else:
                  sentiment = 'neutral'
              
              return {
                  'polarity': round(polarity, 3),
                  'subjectivity': round(blob.sentiment.subjectivity, 3),
                  'sentiment': sentiment
              }
          except:
              return {'polarity': 0, 'subjectivity': 0, 'sentiment': 'neutral'}
      
      def analyze_sentiment_vader(self, text):
          """
          Analyze sentiment using VADER
          
          Returns:
              Dictionary with compound score and sentiment label
          """
          if not text or pd.isna(text):
              return {'compound': 0, 'sentiment': 'neutral'}
          
          try:
              scores = self.vader.polarity_scores(str(text))
              compound = scores['compound']
              
              if compound >= 0.05:
                  sentiment = 'positive'
              elif compound <= -0.05:
                  sentiment = 'negative'
              else:
                  sentiment = 'neutral'
              
              return {
                  'compound': round(compound, 3),
                  'sentiment': sentiment
              }
          except:
              return {'compound': 0, 'sentiment': 'neutral'}
      
      def add_sentiment_analysis(self, df, method='vader', text_column='caption'):
          """
          Add sentiment analysis to DataFrame
          
          Args:
              df: DataFrame with text column
              method: 'vader' or 'textblob'
              text_column: Name of column containing text to analyze
              
          Returns:
              DataFrame with sentiment columns added
          """
          if df.empty or text_column not in df.columns:
              return df
          
          df = df.copy()
          
          if method == 'vader':
              sentiments = df[text_column].apply(self.analyze_sentiment_vader)
              df['sentiment_score'] = sentiments.apply(lambda x: x['compound'])
              df['sentiment_score'] = sentiments.apply(lambda x: x['compound'])
              df['sentiment'] = sentiments.apply(lambda x: x['sentiment'])
          else:
              sentiments = df[text_column].apply(self.analyze_sentiment_textblob)
              df['sentiment_score'] = sentiments.apply(lambda x: x['polarity'])
              df['sentiment'] = sentiments.apply(lambda x: x['sentiment'])
          
          return df
      
      def extract_word_frequency(self, df, top_n=20):
          """
          Extract most common words from captions
          
          Args:
              df: DataFrame with caption column
              top_n: Number of top words to return
              
          Returns:
              List of tuples (word, count)
          """
          if df.empty or 'caption' not in df.columns:
              return []
          
          # Combine all captions
          all_text = ' '.join(df['caption'].dropna().astype(str))
          
          # Remove hashtags, mentions, URLs, and special characters
          all_text = re.sub(r'#\w+', '', all_text)
          all_text = re.sub(r'@\w+', '', all_text)
          all_text = re.sub(r'http\S+', '', all_text)
          all_text = re.sub(r'[^\w\s]', '', all_text)
          
          # Convert to lowercase and split
          words = all_text.lower().split()
          
          # Remove common stop words
          stop_words = {
              'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
              'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
              'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
              'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that',
              'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'my',
              'your', 'his', 'her', 'its', 'our', 'their'
          }
          
          words = [w for w in words if w not in stop_words and len(w) > 2]
          
          # Count frequency
          word_counts = Counter(words)
          
          return word_counts.most_common(top_n)
      
      def aggregate_by_time(self, df, freq='D'):
          """
          Aggregate metrics by time period
          
          Args:
              df: DataFrame with publish_date column
              freq: Pandas frequency string ('H' for hour, 'D' for day, 'W' for week)
              
          Returns:
              DataFrame aggregated by time period
          """
          if df.empty or 'publish_date' not in df.columns:
              return pd.DataFrame()
          
          df = df.copy()
          
          # Ensure publish_date is datetime
          df['publish_date'] = pd.to_datetime(df['publish_date'], errors='coerce')
          df = df.dropna(subset=['publish_date'])
          
          if df.empty:
              return pd.DataFrame()
          
          # Set index and resample
          df_time = df.set_index('publish_date')
          
          aggregated = df_time.resample(freq).agg({
              'video_id': 'count',
              'views': 'sum',
              'likes': 'sum',
              'comments': 'sum',
              'shares': 'sum',
              'saves': 'sum' if 'saves' in df_time.columns else 'max' # max is a safe fallback if column missing
          }).rename(columns={'video_id': 'video_count'})
          
          if 'saves' not in df_time.columns:
              aggregated = aggregated.drop(columns=['saves'], errors='ignore')
          
          return aggregated.reset_index()
      
      def get_top_posts(self, df, metric='engagement_rate', top_n=10):
          """
          Get top performing posts by specified metric
          
          Args:
              df: DataFrame with TikTok data
              metric: Column name to sort by
              top_n: Number of top posts to return
              
          Returns:
              DataFrame with top posts
          """
          if df.empty or metric not in df.columns:
              return pd.DataFrame()
          
          return df.nlargest(top_n, metric)
      
      def get_sentiment_distribution(self, df):
          """
          Get distribution of sentiment categories
          
          Returns:
              Dictionary with sentiment counts
          """
          if df.empty or 'sentiment' not in df.columns:
              return {'positive': 0, 'neutral': 0, 'negative': 0}
          
          sentiment_counts = df['sentiment'].value_counts().to_dict()
          
          # Ensure all categories are present
          return {
              'positive': sentiment_counts.get('positive', 0),
              'neutral': sentiment_counts.get('neutral', 0),
              'negative': sentiment_counts.get('negative', 0)
          }
  
  </file>
</message>





# Response:

## reasoningContent


## text
# Nendo TikTok Bot Coding Style Guide

## 1. Frontend (React & TypeScript)

### Component Patterns
- **Functional Components**: Use `export const ComponentName = (...) => { ... }` rather than function declarations.
- **Conditional Styling**: Use the `cn()` utility (based on `clsx` and `tailwind-merge`) for all dynamic class merging.
- **Prop Interfaces**: Define `interface ComponentProps` immediately above the component. Use `React.ElementType` for props that accept Lucide icons.
- **Icon Rendering**: Render dynamic icons using the `<item.icon />` syntax by assigning the icon to a lowercase variable or property.

### State & Navigation
- **View Management**: Use a `currentView` string and `onNavigate` callback pattern for simple SPA navigation within a dashboard layout.
- **Controlled Collapsibles**: Prefer manual state management (`collapsed`, `setCollapsed`) for sidebars to ensure precise control over layout transitions.

### Project Configuration
- **Path Aliasing**: Use the `@/` prefix to reference the `src` directory, configured via Vite and TypeScript.
- **Vite Plugins**: Use `@vitejs/plugin-react-swc` for faster builds and HMR.

## 2. Python (Streamlit & Scraping)

### Module Architecture
- **Sync/Async Hybrid**: Implement core logic (like scrapers) using `async` methods, but provide `_sync` wrapper functions using `asyncio.run()` for compatibility with Streamlit‚Äôs synchronous execution model.
- **Resource Management**: Use `@st.cache_resource` for singleton-like objects such as database managers (`SheetsManager`) or ML analyzers (`TikTokAnalyzer`).

### Data Handling
- **Pandas Integration**: Return `pd.DataFrame` from all analysis and data retrieval methods.
- **Defensive Mapping**: When parsing external API results (e.g., Apify), use a private `_map_result` method with extensive `.get()` calls and `try/except` blocks to handle inconsistent JSON schemas.
- **Data Deduplication**: Maintain a `set` of unique identifiers (e.g., `video_id`) when appending data to external storage (Google Sheets) to prevent duplicates.

### UI & Visualization
- **Session State**: Explicitly initialize all state variables (e.g., `data_loaded`, `df`) at the start of the `app.py`.
- **Plotly Configuration**: Use `rgba(0,0,0,0)` for `plot_bgcolor` and `paper_bgcolor` to ensure charts blend seamlessly with the Streamlit theme.
- **Progressive Columns**: Use `st.column_config` (e.g., `ProgressColumn`, `LinkColumn`) to turn standard DataFrames into interactive dashboards.

## 3. General Conventions

### External Integrations
- **Google Sheets**: Treat the first row of a worksheet as the schema definition. Automatically initialize headers if `row_values(1)` is empty.
- **Environment Variables**: Use `os.getenv()` as a fallback for API tokens, but prioritize explicit parameter passing for flexibility.

### Error Handling
- **Visual Feedback**: Use `st.sidebar.error` or `st.sidebar.warning` instead of generic print statements for user-facing errors in the dashboard.
- **Silent Failures**: In mapping functions or loops, use `continue` within `try/except` blocks to ensure a single malformed data point doesn't crash the entire ingestion pipeline.

## toolExecutionRequests
[ ]

## metadata
{
  "modelName" : "gemini-3-flash-preview",
  "thinkingTokens" : 0,
  "created" : "2026-02-20 14:34:12",
  "finishReason" : "STOP",
  "serviceTier" : "DEFAULT",
  "inputTokens" : 17861,
  "cachedInputTokens" : 0,
  "outputTokens" : 742,
  "elapsedMs" : 8064
}
